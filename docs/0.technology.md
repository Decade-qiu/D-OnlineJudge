# D-OnlineJudge: 技术选型与架构设计 (v2.0)

本文档阐述 D-OnlineJudge 项目为实现高性能、高可用、高扩展性目标而设计的核心技术体系与架构亮点。

---

## 1. 升级版架构蓝图

为应对复杂的业务场景和未来的技术挑战，D-OnlineJudge 的后端架构演进为一套集流量治理、异步通信、全链路可观测性于一体的现代化微服务体系。

```mermaid
graph TD
    subgraph "用户接入层"
        A[Browser/Client] --> B(API Gateway);
    end

    subgraph "核心业务层"
        direction LR
        B --> D[User Service];
        B --> E[Problem Service];
        B --> F[Submission Service];
    end

    subgraph "核心引擎层"
        K(Sandbox Service)
    end

    subgraph "数据与消息层"
        direction TB
        subgraph "服务治理"
            G[Nacos]
        end
        subgraph "消息与队列"
            I[Redis Task Queue]
            J[RabbitMQ Event Bus]
        end
        subgraph "数据存储"
            H[Redis Cache/Token]
            DB[(MySQL Databases)]
        end
    end
    
    subgraph "可观测性层"
        L[SkyWalking]
        M[Prometheus + Grafana]
        N[ELK/Loki]
    end

    %% 定义服务间的核心交互
    F -- 1. 投递任务 --> I;
    K -- 2. 消费任务 --> I;
    K -- 3. 发布原始结果 --> J;
    F -- 4. 消费结果 & 发布业务事件 --> J;
    D -- 5. 消费用户统计事件 --> J;
    E -- 5. 消费题目统计事件 --> J;
    
    F -- WebSocket 实时推送 --> A;

    %% 定义与中间件的通用交互
    D & E & F & K -- 注册/配置 --> G;
    D & E & F -- 读/写 --> DB;
    D & F -- 读/写 --> H;

    %% 定义与可观测性的交互
    D & E & F & K & B -- 上报数据 --> L;
    D & E & F & K & B -- 上报数据 --> M;
    D & E & F & K & B -- 上报数据 --> N;
```

---

## 2. 核心架构亮点

### 亮点一：统一API网关与智能治理中心

API网关 (`gateway-service`) 不再仅仅是请求的“中转站”，而是升级为系统的“**流量治理与安全中枢**”。

- **统一入口与路由**: 所有外部请求的唯一入口，基于路径将流量精确分发至下游服务。
- **安全屏障**: 
    - **企业级认证**: 采用**长短令牌（Refresh/Access Token）机制**，通过网关层的全局过滤器（GlobalFilter）高效校验短令牌 `access_token`。
    - **服务访问控制**: 在各微服务中配置拦截器，强制要求所有请求必须包含源自网关的特定标识，杜绝了内网环境下的服务横向穿透风险。
- **流量智能治理**: 
    - **动态流量控制**: 集成 **Alibaba Sentinel**，在网关层实现精细化的流量控制，如QPS限制、线程数限制，有效防止恶意请求冲垮后端服务。
    - **熔断与降级**: 对下游服务的调用进行实时监控，当某个服务出现高延迟或频繁错误时，能自动进行熔断，并返回预设的友好响应，防止雪崩效应，保证核心功能的可用性。

### 亮点二：混合消息模式：Redis任务队列与RabbitMQ事件总线

为了同时满足判题任务的“高性能分发”和业务事件的“可靠广播”这两种不同需求，我们采用了一套优雅的混合消息架构，充分利用了 Redis 和 RabbitMQ 各自的优势。

- **Redis List 作为任务队列**: 对于判题任务的分发，我们追求的是极致的低延迟和高吞吐。Redis 基于内存的 `LPUSH` / `BRPOP` 命令提供了完美的轻量级任务队列实现，使得 `sandbox-service` 的提交接口可以瞬时响应。
- **RabbitMQ 作为事件总线**: 对于判题完成后的状态变更，这属于业务领域的事件。我们使用 RabbitMQ 强大的 Topic 交换机来广播这些事件（如 `submission.created`, `problem.solved`），未来任何对这些事件感兴趣的下游服务（如统计服务、徽章服务等）都可以灵活地订阅，实现了真正的事件驱动和微服务解耦。

```mermaid
sequenceDiagram
    autonumber
    participant FE as 前端
    participant Submission as 提交服务
    participant RedisQueue as Redis任务队列
    participant Sandbox as 沙箱服务 (Worker)
    participant RabbitMQ as RabbitMQ事件总线
    participant User as 用户服务
    participant Problem as 题目服务

    FE->>Submission: 1. 提交代码 (POST /submission/create)
    activate Submission
    Submission->>Submission: 2. 创建 Submission 记录 (状态: PENDING)
    Submission->>RedisQueue: 3. LPUSH judging:queue (发布判题任务)
    Submission-->>FE: 4. 立即返回 submissionId
    deactivate Submission

    FE->>Submission: 5. 建立 WebSocket 连接

    activate Sandbox
    Sandbox->>RedisQueue: 6. BRPOP judging:queue (阻塞等待任务)
    Sandbox->>Sandbox: 7. 执行 Docker 判题
    Sandbox->>RabbitMQ: 8. 发送 `judging.result` 消息
    deactivate Sandbox

    activate Submission
    Submission->>RabbitMQ: 9. 监听并消费 `judging.result` 消息
    Submission->>Submission: 10. 更新数据库记录状态
    Submission->>FE: 11. 通过 WebSocket 推送最终结果
    Submission->>RabbitMQ: 12. 发送 `submission.created` & `problem.solved` 业务事件
    deactivate Submission

    activate User
    User->>RabbitMQ: 13. 消费 `problem.solved` 事件并更新用户统计
    deactivate User

    activate Problem
    Problem->>RabbitMQ: 14. 消费 `submission.created` 事件并更新题目统计
    deactivate Problem
```

### 亮点三：企业级认证与分布式缓存中心

我们引入 **Redis** 作为核心的内存数据中间件，并升级了认证体系，实现了高性能与高安全性的统一。

#### 认证与刷新流程

```mermaid
sequenceDiagram
    autonumber
    participant User as 用户
    participant Frontend as 前端
    participant Gateway as 网关
    participant UserService as 用户服务
    participant Redis as Redis

    User->>Frontend: 输入用户名/密码登录
    Frontend->>Gateway: POST /user/login
    Gateway->>UserService: 转发登录请求

    activate UserService
    UserService->>UserService: 校验用户名/密码
    UserService->>UserService: 生成 access_token (JWT)
    UserService->>UserService: 生成 refresh_token (UUID)
    UserService->>Redis: 将 {refresh_token: userId} 存入 Redis (设置过期时间)
    UserService-->>Frontend: 返回 access_token 和 refresh_token
    deactivate UserService

    Note over Frontend: 前端将两个 token 存入持久化存储 (Pinia)

    loop 日常API请求
        Frontend->>Gateway: GET /problem/list (Header携带 access_token)
        activate Gateway
        Gateway->>Gateway: 校验 access_token (JWT)
        alt access_token 有效
            Gateway->>ProblemService: 转发请求 (注入 userId)
        else access_token 过期
            Gateway-->>Frontend: 返回 401 Unauthorized
        end
        deactivate Gateway
    end

    Note over Frontend: 收到 401 错误，触发静默刷新逻辑

    Frontend->>Gateway: POST /user/refresh (Header携带 refresh_token)
    Gateway->>UserService: 转发刷新请求

    activate UserService
    UserService->>Redis: 在 Redis 中查找 refresh_token 是否存在
    alt refresh_token 有效
        UserService->>Redis: 从 Redis 获取 userId
        UserService->>UserService: 生成新的 access_token
        UserService-->>Frontend: 返回新的 access_token
    else refresh_token 无效或过期
        UserService-->>Frontend: 返回 401/403，提示需要重新登录
    end
    deactivate UserService

    Note over Frontend: 收到新 access_token，更新本地存储，并重试刚才失败的API请求
```

- **长短令牌机制**:
    - **`access_token` (JWT)**: 短时效、无状态，用于API请求，包含用户的角色权限信息，由网关进行快速验证。
    - **`refresh_token`**: 长时效、有状态，存储在 **Redis** 中。用于在`access_token`过期后，安全地换取新的令牌，实现了用户的无感续期和强制下线等高级功能。
- **分布式高速缓存**: 
    - **热点数据缓存**: 将频繁访问且不常变化的数据，如用户信息、题目详情、热门排行榜等，缓存在Redis中，大幅降低数据库的读取压力。
    - **分布式锁**: 利用Redis实现分布式锁，解决在集群环境下更新题目通过数、用户积分等场景下的并发安全问题。

### 亮点四：优雅的认证与用户上下文传递

在微服务架构中，如何安全、无感地将用户的身份信息（如 User ID）在服务调用链中传递，是一个核心问题。本项目采用了一套“网关注入 + 线程内缓存 + Feign自动填充”的优雅方案。

```mermaid
sequenceDiagram
    participant Gateway as 网关
    participant ServiceA as 微服务A (e.g., Sandbox)
    participant ServiceB as 微服务B (e.g., Problem)

    Gateway->>ServiceA: 1. 转发请求 (Header 中已包含 `uid`)
    
    activate ServiceA
    ServiceA->>ServiceA: 2. `IdentityInterceptor` 拦截请求
    ServiceA->>ServiceA: 3. 从 Header 读取 `uid` 并存入 `UserContext` (ThreadLocal)
    ServiceA->>ServiceB: 4. 通过 FeignClient 发起内部调用
    deactivate ServiceA

    Note over ServiceA, ServiceB: Feign `RequestInterceptor` 被触发
    ServiceA->>ServiceA: 5. 从 `UserContext` 取出 `uid`
    ServiceA->>ServiceB: 6. 将 `uid` 自动填入新请求的 Header 中

    activate ServiceB
    ServiceB->>ServiceB: 7. `IdentityInterceptor` 再次拦截
    ServiceB->>ServiceB: 8. 验证 Header 中存在 `uid`，放行
    deactivate ServiceB
```

- **网关层统一注入**: 所有外部请求经网关认证后，用户的 `userId` 被解析并放入请求头，作为身份标识的唯一来源。
- **线程内上下文 (`ThreadLocal`)**: 每个业务微服务都配置了一个 `IdentityInterceptor`。它在处理每个请求的开始，从请求头中读取 `userId` 并存入一个 `ThreadLocal` 变量 (`UserContext`) 中。这使得在当前请求的处理线程中，任何地方的业务代码都可以方便地通过 `UserContext.getCurrentUser()` 获取当前用户ID，而无需在方法参数中层层传递。
- **Feign 自动填充**: 为了解决服务间调用时 `ThreadLocal` 信息丢失的问题，我们配置了一个全局的 Feign `RequestInterceptor`。它会在每一次 Feign 调用发起前，自动从 `UserContext` 中获取 `userId`，并将其添加到新的 HTTP 请求头中。这确保了用户身份在整个微服务调用链中能够无缝、安全地传递下去。
- **安全保障**: 每个业务微服务的 `IdentityInterceptor` 都会检查请求头中是否存在 `uid`。如果不存在，说明该请求很可能是未经网关的非法内部调用，服务会直接拒绝该请求，从而保证了所有服务调用都必须通过网关的统一认证。

### 亮点五：全局 WebSocket 实时通知

为了在异步判题架构下，向用户提供优雅、无干扰的实时结果反馈，我们构建了一套全局的 WebSocket 通知系统，取代了需要用户停留在当前页面的传统方案。

```mermaid
sequenceDiagram
    autonumber
    participant FE as 前端
    participant Submission as 提交服务
    participant Redis as Redis Pub/Sub (或 RabbitMQ)

    FE->>Submission: 1. 提交代码后，拿到 submissionId
    FE->>Submission: 2. 建立 WebSocket 连接
    activate Submission
    Submission->>Submission: 3. 存储 Session，并与 submissionId 关联
    deactivate Submission

    Note right of Submission: ... 判题流程在后台异步进行 ...

    activate Submission
    Submission->>Redis: 4. 监听器收到判题结果消息
    Submission->>Submission: 5. 根据 submissionId 查找对应的 Session
    Submission-->>FE: 6. 通过 WebSocket 主动推送结果
    deactivate Submission

    FE->>FE: 7. 收到消息，更新UI（弹出通知）
```

- **全局服务化**: 我们将 WebSocket 的所有连接管理、消息监听和处理逻辑，都封装在一个独立的、全局可用的服务 (`src/utils/websocket.ts`) 中。该服务在应用根组件 (`App.vue`) 挂载时被初始化，确保了其生命周期与整个应用保持一致。

- **按需订阅**: 用户在提交代码后，前端会调用 `useWebSocket().subscribeToSubmission(submissionId)` 方法。该方法会通过已建立的全局 WebSocket 连接，向后端发送一条“订阅”消息，将当前会话与本次提交的 `submissionId` 关联起来。

- **非阻塞式富文本通知**: 当后端判题完成，并通过 WebSocket 推送结果时，前端的全局监听器会捕获到该消息。它并不会粗暴地打断用户的当前操作，而是调用 Element Plus 的 `ElNotification` 组件，在屏幕的右下角弹出一个内容丰富的通知卡片。我们利用 Vue 的 `h` 函数（渲染函数）来动态构建通知的 `message`，使其可以包含状态、耗时、内存、错误信息等复杂的 HTML 结构。

- **可交互与自动关闭**: 该通知卡片在显示一段时间后会自动消失，用户也可以点击它，页面会自动跳转到对应的提交详情页，实现了从“收到结果”到“查看详情”的无缝体验。

### 亮点六：全链路可观测性体系

为了在复杂的微服务环境中快速定位问题、分析性能，我们构建了覆盖“日志、追踪、监控”三大黄金指标的全链路可观测性体系。

- **分布式日志 (ELK / Loki)**: 所有微服务的日志被实时收集、聚合到统一的日志中心，开发者可以通过图形化界面，一站式地搜索和分析所有服务的日志，告别逐台服务器捞日志的原始方式。
- **分布式链路追踪 (Apache SkyWalking)**: 通过字节码注入或Agent技术，自动将一个用户请求在所有微服务间的调用过程串联成一条完整的调用链。当请求变慢或出错时，能一目了然地看到瓶颈或故障发生在哪个服务的哪个环节。
- **立体化监控与告警 (Prometheus + Grafana)**: 
    - **指标采集**: 各微服务通过Actuator暴露丰富的运行时指标（JVM、线程池、连接池、API延迟等）。
    - **数据可视化**: Prometheus负责周期性地抓取并存储这些指标，Grafana则提供了强大的仪表盘功能，将枯燥的数据转化为直观的图表，实现对系统健康状况的实时监控。
    - **智能告警**: 在Grafana中可以配置灵活的告警规则，当系统指标异常时，能通过邮件、钉钉等多种渠道及时通知开发人员。

---

## 3. 架构升级与演进计划

为实现上述架构蓝图，我们制定了清晰的演进计划。后续的开发工作将围绕以下核心任务展开：

- **[ ] 中间件集成**
    - [x] **集成 Redis**: 用于实现长短令牌存储和分布式缓存。
    - [x] **集成 Message Queue (RabbitMQ/RocketMQ)**: 重构判题流程，实现服务间的异步解耦。
    - [ ] **集成 Sentinel**: 在网关层实现流量控制与熔断降级。

- **[ ] 实时通信**
    - [ ] **实现 WebSocket 服务**: 用于向前端实时推送判题结果。

- **[ ] 可观测性建设**
    - [ ] **集成 SkyWalking**: 搭建分布式链路追踪系统。
    - [ ] **部署 Prometheus & Grafana**: 建立统一的指标监控与告警平台。
    - [ ] **部署 ELK 或 Loki**: 建立集中式的日志管理系统。

- **[ ] 自动化运维**
    - [ ] **建立 CI/CD 流水线**: 使用 Jenkins 或 GitHub Actions 自动化项目的测试、构建和部署流程。

---

## 附录：关键技术与设计决策

### 1. 全局统一异常处理

为了避免在每个 Controller 方法中都使用 `try-catch` 来处理异常，并向前端返回统一、规范的错误格式，项目采用了 Spring 的全局异常处理机制。

- **实现**: 通过创建一个带有 `@RestControllerAdvice` 注解的类，并为不同类型的异常（如我们自定义的 `BadRequestException`, `UnauthorizedException`，以及通用的 `Exception`）编写被 `@ExceptionHandler` 注解标记的方法。
- **优势**: 所有的业务异常都会被这个切面捕获，并被统一包装成 JSON 格式返回给前端。这极大地简化了业务代码，并保证了 API 错误响应的一致性。

### 2. 双配置文件启动模型 (`bootstrap` vs `application`)

在 Spring Cloud 环境中，我们为每个微服务都配置了 `bootstrap.yaml` 和 `application.yaml` 两个文件，它们在应用启动时有不同的加载时机和用途。

- **`bootstrap.yaml` (引导上下文)**:
    - **加载时机**: **优先于** `application.yaml` 加载。
    - **核心用途**: 主要用于存放那些需要**引导应用、发现配置中心**的配置。在本项目中，它只存放两项最关键的信息：
        1.  `spring.application.name`: 应用的名称，用于服务注册和配置拉取。
        2.  `spring.cloud.nacos.server-addr`: Nacos 服务器的地址。
    - **工作流程**: Spring Cloud 应用启动时，会先创建一个“引导上下文”，加载 `bootstrap.yaml`。通过这里面的配置，它知道了自己是谁、要去哪里找配置中心。然后，它会去 Nacos 拉取所有共享和私有的配置。

- **`application.yaml` (应用上下文)**:
    - **加载时机**: 在 `bootstrap` 阶段**之后**加载。
    - **核心用途**: 用于存放应用的**常规业务配置**，例如服务器端口 (`server.port`)、数据库地址、Swagger 扫描路径等。这些配置的值可以被 Nacos 中的同名配置所覆盖。

通过这种分离，我们确保了应用能够先找到“大本营”（Nacos），然后再从“大本营”获取详细的“作战计划”（业务配置），流程清晰且可靠。